{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02662f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aneesh\\anaconda3\\envs\\vision_models\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPImageProcessor\n",
    "from diffusers import AutoencoderTiny\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import decode_image\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torch.nn import TripletMarginLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d71bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoencoderTiny(\n",
       "  (encoder): EncoderTiny(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (3): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (4): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (5): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (7): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (8): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (9): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (11): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (12): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (13): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (14): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderTiny(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (3): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (4): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (5): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (8): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (9): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (12): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (13): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (14): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (15): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (17): AutoencoderTinyBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU()\n",
       "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (skip): Identity()\n",
       "        (fuse): ReLU()\n",
       "      )\n",
       "      (18): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\", torch_dtype=torch.float32)\n",
    "vae.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71af6efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata_fp = \"scene_data/train-scene classification/train/\"\\n\\nimg = Image.open(data_fp + \"5.jpg\")\\nprocessed = processor(img, return_tensors = \\'pt\\')\\npixel_values = processed[\\'pixel_values\\'].to(vae.device).to(dtype=vae.dtype)\\nwith torch.no_grad():\\n    latents = vae.encode(pixel_values).latents\\n\\n\\nprint(\"Latent shape:\", latents.shape)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data_fp = \"scene_data/train-scene classification/train/\"\n",
    "\n",
    "img = Image.open(data_fp + \"5.jpg\")\n",
    "processed = processor(img, return_tensors = 'pt')\n",
    "pixel_values = processed['pixel_values'].to(vae.device).to(dtype=vae.dtype)\n",
    "with torch.no_grad():\n",
    "    latents = vae.encode(pixel_values).latents\n",
    "\n",
    "\n",
    "print(\"Latent shape:\", latents.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe3d64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndecoded_image = vae.decode(latents).sample\\ntensor_to_pil = transforms.ToPILImage()\\npil_image = tensor_to_pil(decoded_image[0])\\n\\npil_image\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "decoded_image = vae.decode(latents).sample\n",
    "tensor_to_pil = transforms.ToPILImage()\n",
    "pil_image = tensor_to_pil(decoded_image[0])\n",
    "\n",
    "pil_image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0d37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTripletDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor = self.img_labels.iloc[idx]\n",
    "        positive = self.img_labels[self.img_labels['label'] == anchor['label']].sample(1).iloc[0]\n",
    "        negative = self.img_labels[self.img_labels['label'] != anchor['label']].sample(1).iloc[0]\n",
    "\n",
    "        anchor_image = Image.open(os.path.join(self.img_dir, anchor['image_name']))\n",
    "        positive_image = Image.open(os.path.join(self.img_dir, positive['image_name']))\n",
    "        negative_image = Image.open(os.path.join(self.img_dir, negative['image_name']))\n",
    "\n",
    "        a_processed = processor(anchor_image, return_tensors = 'pt')\n",
    "        p_processed = processor(positive_image, return_tensors = 'pt')\n",
    "        n_processed = processor(negative_image, return_tensors = 'pt')\n",
    "\n",
    "        anchor_tensor = a_processed['pixel_values'].to(dtype=vae.dtype)\n",
    "        positive_tensor = p_processed['pixel_values'].to(dtype=vae.dtype)\n",
    "        negative_tensor = n_processed['pixel_values'].to(dtype=vae.dtype)\n",
    "\n",
    "        return anchor_tensor[0], positive_tensor[0], negative_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b034b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = decode_image(img_path)\n",
    "\n",
    "        # Define the desired output size (e.g., 224x224)\n",
    "        desired_size = (150, 150)\n",
    "\n",
    "        # Create a Resize transform\n",
    "        resize_transform = transforms.Resize(desired_size)\n",
    "\n",
    "        # Apply the transform to the decoded image\n",
    "        resized_image_tensor = resize_transform(image)\n",
    "\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            resized_image_tensor = self.transform(resized_image_tensor)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return resized_image_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0c1231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"scene_data/train-scene classification/train/\"\n",
    "train_annotations = \"scene_data/train-scene classification/train.csv\"\n",
    "test_annotations = \"scene_data/test.csv\"\n",
    "\n",
    "training_data = CustomImageDataset(train_annotations, img_dir)\n",
    "test_data = CustomImageDataset(test_annotations, img_dir)\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "triplet_data = ImageTripletDataset(train_annotations, img_dir)\n",
    "triplet_dataloader = DataLoader(triplet_data, batch_size=64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7155ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Display image and label.\\nanchors, positives, negatives = next(iter(triplet_dataloader))\\nprint(f\"Feature batch shape: {anchors.size()}\")\\na_img = anchors[0].squeeze().permute(1,2,0)\\nplt.imshow(a_img, cmap=\"gray\")\\nplt.show()\\n\\np_img = positives[0].squeeze().permute(1,2,0)\\nplt.imshow(p_img, cmap=\"gray\")\\nplt.show()\\n\\nn_img = negatives[0].squeeze().permute(1,2,0)\\nplt.imshow(n_img, cmap=\"gray\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Display image and label.\n",
    "anchors, positives, negatives = next(iter(triplet_dataloader))\n",
    "print(f\"Feature batch shape: {anchors.size()}\")\n",
    "a_img = anchors[0].squeeze().permute(1,2,0)\n",
    "plt.imshow(a_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "p_img = positives[0].squeeze().permute(1,2,0)\n",
    "plt.imshow(p_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "n_img = negatives[0].squeeze().permute(1,2,0)\n",
    "plt.imshow(n_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0207f05",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "criterion = TripletMarginLoss(margin=1.0, p=2) # p=2 for Euclidean distance\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa9781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/267], Loss: 0.7947\n",
      "Epoch [1/10], Step [20/267], Loss: 0.5907\n",
      "Epoch [1/10], Step [30/267], Loss: 0.4988\n",
      "Epoch [1/10], Step [40/267], Loss: 0.4655\n",
      "Epoch [1/10], Step [50/267], Loss: 0.3719\n",
      "Epoch [1/10], Step [60/267], Loss: 0.4459\n",
      "Epoch [1/10], Step [70/267], Loss: 0.4961\n",
      "Epoch [1/10], Step [80/267], Loss: 0.4577\n",
      "Epoch [1/10], Step [90/267], Loss: 0.4446\n",
      "Epoch [1/10], Step [100/267], Loss: 0.5730\n",
      "Epoch [1/10], Step [110/267], Loss: 0.5775\n",
      "Epoch [1/10], Step [120/267], Loss: 0.5857\n",
      "Epoch [1/10], Step [130/267], Loss: 0.5175\n",
      "Epoch [1/10], Step [140/267], Loss: 0.5570\n",
      "Epoch [1/10], Step [150/267], Loss: 0.4979\n",
      "Epoch [1/10], Step [160/267], Loss: 0.3647\n",
      "Epoch [1/10], Step [170/267], Loss: 0.5392\n",
      "Epoch [1/10], Step [180/267], Loss: 0.4915\n",
      "Epoch [1/10], Step [190/267], Loss: 0.5509\n",
      "Epoch [1/10], Step [200/267], Loss: 0.2494\n",
      "Epoch [1/10], Step [210/267], Loss: 0.4553\n",
      "Epoch [1/10], Step [220/267], Loss: 0.4466\n",
      "Epoch [1/10], Step [230/267], Loss: 0.5243\n",
      "Epoch [1/10], Step [240/267], Loss: 0.2828\n",
      "Epoch [1/10], Step [250/267], Loss: 0.3648\n",
      "Epoch [1/10], Step [260/267], Loss: 0.4230\n",
      "Epoch [2/10], Step [10/267], Loss: 0.4704\n",
      "Epoch [2/10], Step [20/267], Loss: 0.5885\n",
      "Epoch [2/10], Step [30/267], Loss: 0.3714\n",
      "Epoch [2/10], Step [40/267], Loss: 0.4732\n",
      "Epoch [2/10], Step [50/267], Loss: 0.5406\n",
      "Epoch [2/10], Step [60/267], Loss: 0.4944\n",
      "Epoch [2/10], Step [70/267], Loss: 0.3787\n",
      "Epoch [2/10], Step [80/267], Loss: 0.4114\n",
      "Epoch [2/10], Step [90/267], Loss: 0.6093\n",
      "Epoch [2/10], Step [100/267], Loss: 0.5266\n",
      "Epoch [2/10], Step [110/267], Loss: 0.4444\n",
      "Epoch [2/10], Step [120/267], Loss: 0.4646\n",
      "Epoch [2/10], Step [130/267], Loss: 0.4648\n",
      "Epoch [2/10], Step [140/267], Loss: 0.4617\n",
      "Epoch [2/10], Step [150/267], Loss: 0.5024\n",
      "Epoch [2/10], Step [160/267], Loss: 0.3772\n",
      "Epoch [2/10], Step [170/267], Loss: 0.5042\n",
      "Epoch [2/10], Step [180/267], Loss: 0.4262\n",
      "Epoch [2/10], Step [190/267], Loss: 0.4580\n",
      "Epoch [2/10], Step [200/267], Loss: 0.4175\n",
      "Epoch [2/10], Step [210/267], Loss: 0.4266\n",
      "Epoch [2/10], Step [220/267], Loss: 0.4053\n",
      "Epoch [2/10], Step [230/267], Loss: 0.3991\n",
      "Epoch [2/10], Step [240/267], Loss: 0.5094\n",
      "Epoch [2/10], Step [250/267], Loss: 0.2823\n",
      "Epoch [2/10], Step [260/267], Loss: 0.4372\n",
      "Epoch [3/10], Step [10/267], Loss: 0.3333\n",
      "Epoch [3/10], Step [20/267], Loss: 0.2274\n",
      "Epoch [3/10], Step [30/267], Loss: 0.3148\n",
      "Epoch [3/10], Step [40/267], Loss: 0.4455\n",
      "Epoch [3/10], Step [50/267], Loss: 0.2704\n",
      "Epoch [3/10], Step [60/267], Loss: 0.3788\n",
      "Epoch [3/10], Step [70/267], Loss: 0.3227\n",
      "Epoch [3/10], Step [80/267], Loss: 0.4894\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (anchor_data, positive_data, negative_data) in enumerate(triplet_dataloader):\n",
    "        # Get embeddings from the model\n",
    "\n",
    "        anchor_embedding = vae.encode(anchor_data.to(vae.device)).latents\n",
    "        positive_embedding = vae.encode(positive_data.to(vae.device)).latents\n",
    "        negative_embedding = vae.encode(negative_data.to(vae.device)).latents\n",
    "\n",
    "        # Calculate Triplet Margin Loss\n",
    "        loss = criterion(anchor_embedding, positive_embedding, negative_embedding)\n",
    "\n",
    "        # Backpropagation and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(triplet_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de8980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
